{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#262626; text-align: center;\">\n",
    "<img src=\"https://fiapfunctions.blob.core.windows.net/datasets/capa.png\">\n",
    "</div>\n",
    "\n",
    "# Speech Gender Detection\n",
    "\n",
    "O SINAL DE VOZ\n",
    "\n",
    "### Conteúdo linguístico:\n",
    "* linguagem\n",
    "* semântica\n",
    "* sotaque\n",
    "\n",
    "### Conteúdo paralinguístico:\n",
    "* identidade\n",
    "* sexo\n",
    "* idade\n",
    "* estado emocional\n",
    "\n",
    "### Outras informações:\n",
    "* ambiente da gravação\n",
    "* qualidade da gravação\n",
    "* ruído\n",
    "* canal de transmissão\n",
    "* localização (mais de 1 canal)\n",
    "\n",
    "---\n",
    "\n",
    "As cordas vocais modulam o fluxo de ar, abrindo e fechando rapidamente a passagem pela laringe. Sua taxa de vibração é determinada principalmente por sua massa e tensão, embora a pressão e velocidade do ar também contribuam, em menor escala. Durante uma fala normal, a taxa de vibração das pregas vocais pode variar a uma razão de 2:1 (uma oitava).\n",
    "\n",
    "## Freqüências típicas\n",
    "* Homem: 110 Hz; \n",
    "* Mulher: 220 Hz; \n",
    "* Crianças: 300 Hz.\n",
    "\n",
    "ref: [Física da Fala e da Audição](https://sites.ifi.unicamp.br/graduacao/files/2018/06/M%C3%B3dulo-09_A-Voz-Humana-I_Produ%C3%A7%C3%A3o-da-Fala.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://meriatdatasets.blob.core.windows.net/public/digital-audio-processing/OSR_uk_000_0022_8k.wav -o audios/OSR_uk_000_0022_8k.wav\n",
    "!curl https://meriatdatasets.blob.core.windows.net/public/digital-audio-processing/OSR_us_000_0019_8k.wav -o audios/OSR_us_000_0019_8k.wav\n",
    "!curl https://meriatdatasets.blob.core.windows.net/public/digital-audio-processing/wrapping_paper.wav -o audios/wrapping_paper.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_vad(wav_file):\n",
    "    fs, audio = wavfile.read(wav_file)\n",
    "    audio = audio / 2.0 ** 14 # normalize in range (-2, +2)\n",
    "    audio = audio - np.mean(audio)\n",
    "\n",
    "    frame_size_ms = 25\n",
    "    step_size_ms = 10\n",
    "    frame_size_samples = int(fs * frame_size_ms / 1000)\n",
    "    step_size_samples = int(fs * step_size_ms / 1000)\n",
    "    number_of_frames = int((len(audio) - frame_size_samples) / step_size_samples)\n",
    "    total_number_samples = int (\n",
    "        (number_of_frames * step_size_ms + (frame_size_ms - step_size_ms)) * fs / 1000)\n",
    "    audio = audio[0: total_number_samples] # remove the last partial frame\n",
    "    n_fft_points = 2\n",
    "    while n_fft_points < frame_size_samples:\n",
    "        n_fft_points = n_fft_points *2\n",
    "    pitch_low = 100  # Hz\n",
    "    pitch_high = 300 # Hz\n",
    "    hz_per_bin = fs / n_fft_points\n",
    "    bin0 = int(pitch_low / hz_per_bin)\n",
    "    bin1 = math.ceil(pitch_high / hz_per_bin) + 1\n",
    "    energy_array=[]\n",
    "    for i in np.arange(number_of_frames):\n",
    "        pos0 = int(i * step_size_samples) \n",
    "        pos1 = int(pos0 + frame_size_samples)\n",
    "        frame = audio[pos0 : pos1]\n",
    "        f_audio = abs(np.fft.fft(frame, n=n_fft_points))/float(n_fft_points)\n",
    "        energy_array.append(float(np.mean(f_audio[bin0 : bin1])))\n",
    "    energy_array = np.array(energy_array)\n",
    "    \n",
    "    WT = 500 # ms: window size, treat it as silence if more than 0.5s\n",
    "    L = int(WT/step_size_ms)\n",
    "    noise = 1 << 31;\n",
    "    for n in range(number_of_frames - L):\n",
    "        m = np.mean(energy_array[n : (n+1)*L])\n",
    "        if m < noise:\n",
    "            noise = m\n",
    "\n",
    "    masks = list(range(number_of_frames))\n",
    "    threshold = 6 * noise # noise level, 20*log10(10) = 20dB, 20*log10(6) = 16dB\n",
    "    if number_of_frames * step_size_ms < 6*WT: # too short to find silence noise\n",
    "        a = np.amin(energy_array)\n",
    "        b = np.amax(energy_array)\n",
    "        c = a + (b-a) * 1.0/3.0\n",
    "        if threshold > c:\n",
    "            threshold = c\n",
    "\n",
    "    for i in range(number_of_frames):\n",
    "        if energy_array[i] > threshold:\n",
    "            masks[i] = 1\n",
    "        else:\n",
    "            masks[i] = 0\n",
    "\n",
    "    # check 0s\n",
    "    f0 = 0\n",
    "    for f in range(number_of_frames):\n",
    "        if masks[f] > 0:\n",
    "            f0 = f\n",
    "            break\n",
    "    c = 0\n",
    "    temp = []\n",
    "    for fm in range(f0, number_of_frames):\n",
    "        if masks[fm] > 0:\n",
    "            if c == 0:\n",
    "                continue\n",
    "            if c > 0:\n",
    "                temp.append(fm) # 2nd\n",
    "                c = 0;\n",
    "        else:\n",
    "            if c == 0:\n",
    "                temp.append(fm) # 1st\n",
    "            c += 1\n",
    "    for i in range(int(len(temp)/2)):\n",
    "        p1 = temp[2*i+0]\n",
    "        p2 = temp[2*i+1]\n",
    "        if p2 - p1 < L: # not silence\n",
    "            for p in range(p1, p2+1):\n",
    "                masks[p] = 1\n",
    "    # check 1s\n",
    "    c = 0\n",
    "    temp = []\n",
    "    for fm in range(number_of_frames):\n",
    "        if masks[fm] < 1:\n",
    "            if c == 0:\n",
    "                continue\n",
    "            if c > 0:\n",
    "                temp.append(fm) # 2nd\n",
    "                c = 0;\n",
    "        else:\n",
    "            if c == 0:\n",
    "                temp.append(fm) # 1st\n",
    "            c += 1\n",
    "    for i in range(int(len(temp)/2)):\n",
    "        p1 = temp[2*i+0]\n",
    "        p2 = temp[2*i+1]\n",
    "        if p2 - p1 < int(L/5): # not active\n",
    "            for p in range(p1, p2+1):\n",
    "                masks[p] = 0\n",
    "\n",
    "    vad_masks = np.repeat(masks, step_size_samples)\n",
    "    return (audio[0:len(vad_masks)], vad_masks, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_pitch(wav_file):\n",
    "\n",
    "    audio, vad_masks, fs = speech_vad(wav_file)\n",
    "\n",
    "    frame_size_ms = 25*2\n",
    "    step_size_ms = 10\n",
    "    frame_size_samples = int(fs * frame_size_ms / 1000)\n",
    "    step_size_samples = int(fs * step_size_ms / 1000)\n",
    "    number_of_frames = int((len(audio) - frame_size_samples) / step_size_samples)\n",
    "    total_number_samples = int ((number_of_frames * step_size_ms \n",
    "                                 + (frame_size_ms - step_size_ms)) * fs / 1000)\n",
    "    audio = audio[0: total_number_samples] # remove the last partial frame\n",
    "    n_fft_points = 2\n",
    "    while n_fft_points < frame_size_samples:\n",
    "        n_fft_points = n_fft_points *2\n",
    "    while(n_fft_points * 5 < fs): # no more than 10 Hz per freq bin\n",
    "        n_fft_points = n_fft_points*2\n",
    "    pitch_low = 0  # Hz\n",
    "    pitch_high = 500 # Hz\n",
    "    hz_per_bin = fs / n_fft_points\n",
    "    b0 = int(pitch_low / hz_per_bin)\n",
    "    b1 = math.ceil(pitch_high / hz_per_bin) + 1\n",
    "    energy_array=[]\n",
    "    for i in np.arange(number_of_frames):\n",
    "        pos0 = int(i * step_size_samples) \n",
    "        pos1 = int(pos0 + frame_size_samples)\n",
    "        frame = audio[pos0 : pos1]\n",
    "        f_audio = abs(np.fft.fft(frame, n=n_fft_points))/float(n_fft_points)\n",
    "        energy_array.append(np.array(list(f_audio[b0 : b1])))\n",
    "    energy_array = np.array(energy_array)\n",
    "\n",
    "    filter_bank = np.zeros(b1-b0, dtype = float)\n",
    "    for i in np.arange(number_of_frames):\n",
    "        m = i * step_size_samples\n",
    "        n = i * step_size_samples + int(frame_size_samples)\n",
    "        if vad_masks[m] > 0 and vad_masks[n] > 0:\n",
    "            filter_bank = filter_bank + energy_array[i]\n",
    "    xf = np.arange(0, len(filter_bank), 1) * fs / n_fft_points\n",
    "    x = np.arange(0, len(filter_bank), 1) * fs / n_fft_points\n",
    "    y = filter_bank\n",
    "    x2 = np.arange(0, len(filter_bank), 0.2) * fs / n_fft_points \n",
    "    z = np.polyfit(x, y, 15)\n",
    "    p = np.poly1d(z)\n",
    "    y2 = p(x2)\n",
    "    \n",
    "    f_low = 85 # Hz\n",
    "    f_high = 285 # Hz\n",
    "    idx1 = int(f_low / fs * n_fft_points)\n",
    "    idx2 = int(f_high / fs * n_fft_points)\n",
    "    maxm = np.amax(filter_bank[0 : idx1])\n",
    "    F0 = 0\n",
    "    for i in np.arange(idx1, (idx2 - idx1), 1):\n",
    "        if (filter_bank[i] > 1.25*maxm and filter_bank[i] > filter_bank[i-1]\n",
    "            and filter_bank[i] > filter_bank[i+1]):\n",
    "            F0 = int(i * (fs / n_fft_points))\n",
    "            break\n",
    "    gender = 'Unknown'\n",
    "    if 85 <= F0 < 170:     # [85, 170)\n",
    "        gender = 'Male'\n",
    "    elif 170 <= F0 < 255: # [170, 255]\n",
    "        gender = 'Female'\n",
    "\n",
    "    return (audio[0:len(vad_masks)], vad_masks, fs, filter_bank, n_fft_points, F0, gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pitch(audio, vad_masks, fs, filter_bank, fft_points, F0, gender):\n",
    "    # pitch estimation and gender detection\n",
    "    xt = np.arange(0, len(audio), 1) / fs *1000\n",
    "    xf = np.arange(0, len(filter_bank), 1) * fs / fft_points \n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(xt, audio)\n",
    "    plt.xlabel('time (ms)')\n",
    "    plt.ylabel('amplitude')\n",
    "    plt.title('audio signal')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(xf, filter_bank)\n",
    "    plt.axvline(x=F0,color='red')\n",
    "    plt.axvline(x=170,color='black',linestyle='dashed')\n",
    "    plt.xlabel('frequency (Hz)')\n",
    "    plt.ylabel('magnitude')\n",
    "    textstr = f'\\nPitch: {str(F0)} Gender: {gender}'\n",
    "    plt.title('pitch estimation')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "audio_file = 'audios/OSR_uk_000_0022_8k.wav'\n",
    "audio, vad_masks, fs, filter_bank, fft_points, F0, gender = speech_pitch(audio_file)\n",
    "print(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pitch(audio, vad_masks, fs, filter_bank, fft_points, F0, gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "audio_file = 'audios/OSR_us_000_0019_8k.wav'\n",
    "audio, vad_masks, fs, filter_bank, fft_points, F0, gender = speech_pitch(audio_file)\n",
    "print(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pitch(audio, vad_masks, fs, filter_bank, fft_points, F0, gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "audio_file = 'audios/wrapping_paper.wav'\n",
    "audio, vad_masks, fs, filter_bank, fft_points, F0, gender = speech_pitch(audio_file)\n",
    "print(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pitch(audio, vad_masks, fs, filter_bank, fft_points, F0, gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
